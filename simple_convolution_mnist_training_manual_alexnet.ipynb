{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import mnist\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4189,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "fashion_mnist =keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_images = np.expand_dims(test_images, axis=3)\n",
    "train_images = np.expand_dims(train_images, axis=3)\n",
    "train_images = train_images/255.0\n",
    "test_images = test_images/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    def __init__(self, name, num_filters, kernel_size=3, padding=((1,1),(1,1)), stride=(1,1)):\n",
    "        self.name = name\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_filters = num_filters        \n",
    "        \n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.filters=None\n",
    "        self.bias=None\n",
    "        with open('initial_weights_lenet/'+self.name+'.weight', 'rb') as fp:\n",
    "            variables = pickle.load(fp)\n",
    "            self.filters = variables[0].numpy()\n",
    "            self.bias = variables[1].numpy()\n",
    "            \n",
    "        \n",
    "    def get_input_pixel(self, image, in_row, in_col, in_ch, height, width):\n",
    "        row = in_row - self.padding[0][0]\n",
    "        col = in_col - self.padding[1][0]\n",
    "        if (row<0 or col<0 or row >= height or col >= width):\n",
    "            return np.zeros((image.shape[0]))\n",
    "        else:\n",
    "            return image[...,row,col,in_ch]\n",
    "\n",
    "    def iterate_regions(self, image, out_h, out_w):\n",
    "   \n",
    "        \n",
    "        im_region = np.zeros((image.shape[0],self.kernel_size, self.kernel_size, image.shape[3]))\n",
    "\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                for k_h in range(self.kernel_size):\n",
    "                    for k_w in range(self.kernel_size):\n",
    "                        in_h = k_h + i*self.stride[0]\n",
    "                        in_w = k_w + j*self.stride[1]\n",
    "                        for ch in range(image.shape[3]):                            \n",
    "                            im_region[:,k_h,k_w,ch] = self.get_input_pixel(image, \n",
    "                                                                      in_h, in_w, ch, \n",
    "                                                                      image.shape[1], image.shape[2])\n",
    "                yield im_region, i, j\n",
    "                \n",
    "    def forward(self, input):\n",
    "        in_n, in_h, in_w, in_ch = input.shape\n",
    "        \n",
    "        #self.filters = np.random.randn(self.num_filters, self.kernel_size, self.kernel_size, \n",
    "         #                              in_ch) / 9\n",
    "        self.filters = np.array(self.filters)\n",
    "        self.bias = np.array(self.bias)\n",
    "       \n",
    "        u_pad = self.padding[0][0]\n",
    "        d_pad = self.padding[0][1]\n",
    "        l_pad = self.padding[1][0]\n",
    "        r_pad = self.padding[1][1]\n",
    "                                        \n",
    "        out_h = int((in_h + u_pad + d_pad - self.kernel_size)//self.stride[0] + 1)\n",
    "        out_w = int((in_w + l_pad + r_pad - self.kernel_size)//self.stride[1] + 1)\n",
    "        \n",
    "        output = np.zeros((in_n, out_h, out_w, self.num_filters))\n",
    "        self.last_input = input\n",
    "        \n",
    "\n",
    "        for im_region, i, j in self.iterate_regions(input, out_h, out_w):\n",
    "            for ch in range(self.num_filters):\n",
    "                output[:,i,j,ch] = np.sum(im_region * \n",
    "                                          self.filters[...,ch], \n",
    "                                          axis=(1,2,3))+self.bias[ch]\n",
    "        output=np.maximum(output,0)\n",
    "            \n",
    "        return output\n",
    "    \n",
    "    def backward(self, dL_dout, out, learning_rate,update=True):        \n",
    "        dact_dconv = np.ones(dL_dout.shape) \n",
    "        dact_dconv = np.where(out>0, 1, 0)\n",
    "        \n",
    "\n",
    "        dL_dconv = dL_dout * dact_dconv\n",
    "        in_n, in_h, in_w, in_ch = self.last_input.shape\n",
    "        u_pad = self.padding[0][0]\n",
    "        d_pad = self.padding[0][1]\n",
    "        l_pad = self.padding[1][0]\n",
    "        r_pad = self.padding[1][1]\n",
    "        out_h = int((in_h + u_pad + d_pad - self.kernel_size)//self.stride[0] + 1)\n",
    "        out_w = int((in_w + l_pad + r_pad - self.kernel_size)//self.stride[1] + 1)\n",
    "        \n",
    "        dL_dw = np.zeros((self.last_input.shape[0],\n",
    "                          self.filters.shape[0], self.filters.shape[1], \n",
    "                          self.filters.shape[2], self.filters.shape[3]))\n",
    "        \n",
    "        dL_din = np.zeros(self.last_input.shape)\n",
    "        \n",
    "        dL_dbias = np.zeros((self.last_input.shape[0],self.bias.shape[0]))\n",
    "        \n",
    "        for im_region, i, j in self.iterate_regions(self.last_input, out_h, out_w):\n",
    "            origin_start_in_h = i*self.stride[0]-u_pad\n",
    "            origin_end_in_h = origin_start_in_h + self.kernel_size\n",
    "            start_kernel_h = - origin_start_in_h if origin_start_in_h < 0 else 0\n",
    "            end_kernel_h = self.kernel_size - (origin_end_in_h - in_h) if origin_end_in_h > in_h else self.kernel_size\n",
    "            \n",
    "            origin_start_in_w = j*self.stride[0]-l_pad\n",
    "            origin_end_in_w = origin_start_in_w + self.kernel_size\n",
    "            start_kernel_w = - origin_start_in_w if origin_start_in_w < 0 else 0\n",
    "            end_kernel_w = self.kernel_size - (origin_end_in_w - in_w) if origin_end_in_w > in_w else self.kernel_size\n",
    "            \n",
    "            start_in_h = max(origin_start_in_h,0)\n",
    "            end_in_h = min(origin_end_in_h,in_h)\n",
    "            start_in_w = max(origin_start_in_w,0)\n",
    "            end_in_w = min(origin_end_in_w,in_w)\n",
    "            \n",
    "            #print('in_h:{}'.format(in_h))\n",
    "            #print('origin_start_in_h:{}, origin_end_in_h:{}'.format(origin_start_in_h, origin_end_in_h))\n",
    "            #print('origin_start_in_w:{}, origin_end_in_w:{}'.format(origin_start_in_w, origin_end_in_w))\n",
    "            #print('start_in_h:{}, end_in_h:{}'.format(start_in_h, end_in_h))\n",
    "            #print('start_in_w:{}, end_in_w:{}'.format(start_in_w, end_in_w))\n",
    "            #print('start_kernel_h:{}, end_kernel_h:{}'.format(start_kernel_h, end_kernel_h))\n",
    "            #print('start_kernel_w:{}, end_kernel_w:{}\\n'.format(start_kernel_w, end_kernel_w))\n",
    "            \n",
    "            for n in range(im_region.shape[0]):\n",
    "                for f in range(self.num_filters):\n",
    "                    dL_dw[n,...,f] += dL_dconv[n,i,j,f] * im_region[n,...]\n",
    "                    in_h = i*self.stride[0]\n",
    "                    in_w = j*self.stride[1]\n",
    "                    \n",
    "                    \n",
    "                    #dL_din[n,in_h:in_h+self.kernel_size, in_w: in_w+self.kernel_size,:] += dL_dconv[n,i,j,f] * self.filters[...,f]\n",
    "                    dL_din[n,start_in_h:end_in_h, start_in_w:end_in_w,:] += dL_dconv[n,i,j,f] * self.filters[start_kernel_h:end_kernel_h, start_kernel_w:end_kernel_w,:,f]\n",
    "                    \n",
    "        \n",
    "        \n",
    "        dL_dbias[...] = np.sum(dL_dconv[...], axis=(0,1,2))\n",
    "        if self.name == 'conv1':\n",
    "            print('{} filters'.format(self.name))\n",
    "            print(self.bias)\n",
    "            print('grads')\n",
    "            print(dL_dbias)\n",
    "        if update:\n",
    "            for n in range(dL_dw.shape[0]):\n",
    "                self.filters -= learning_rate * dL_dw[n]\n",
    "                self.bias -= learning_rate * dL_dbias[n]\n",
    "\n",
    "        return dL_din\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1 1 2 1]\n",
      "  [2 2 3 2]\n",
      "  [3 3 4 3]]\n",
      "\n",
      " [[1 1 5 1]\n",
      "  [2 2 6 2]\n",
      "  [3 3 7 3]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([12, 12, 27, 12])"
      ]
     },
     "execution_count": 4192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[[1,1,2,1], [2,2,3,2], [3,3,4,3]],[[1,1,5,1], [2,2,6,2], [3,3,7,3]]])\n",
    "print(a)\n",
    "np.sum(a,axis=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4193,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    def __init__(self, name,kernel_size=2,padding=((1,1),(1,1)), stride=(1,1)):\n",
    "        self.name = name\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        \n",
    "    def get_input_pixel(self, image, in_row, in_col, in_ch, height, width):\n",
    "        row = in_row - self.padding[0][0]\n",
    "        col = in_col - self.padding[1][0]\n",
    "        if (row<0 or col<0 or row >= height or col >= width):\n",
    "            return np.zeros((image.shape[0]))\n",
    "        else:\n",
    "            return image[...,row,col,in_ch]\n",
    "\n",
    "    def iterate_regions(self, image, out_h, out_w, out_ch):   \n",
    "        \n",
    "        im_region = np.zeros((image.shape[0],self.kernel_size, self.kernel_size))\n",
    "\n",
    "        ch = out_ch\n",
    "        for i in range(out_h):\n",
    "            for j in range(out_w):\n",
    "                for k_h in range(self.kernel_size):\n",
    "                    for k_w in range(self.kernel_size):\n",
    "                        in_h = k_h + i*self.stride[0]\n",
    "                        in_w = k_w + j*self.stride[1]                           \n",
    "                        im_region[...,k_h,k_w] = self.get_input_pixel(image, \n",
    "                                                                      in_h, in_w, ch, \n",
    "                                                                      image.shape[1], image.shape[2])\n",
    "                yield im_region, i, j, ch\n",
    "                \n",
    "    def forward(self, input):\n",
    "        in_n,in_h, in_w, in_ch = input.shape\n",
    "        u_pad = self.padding[0][0]\n",
    "        d_pad = self.padding[0][1]\n",
    "        l_pad = self.padding[1][0]\n",
    "        r_pad = self.padding[1][1]\n",
    "        \n",
    "        self.last_input = input\n",
    "        self.input_shape = input.shape\n",
    "        out_h = int((in_h + u_pad + d_pad - self.kernel_size)//self.stride[0] + 1)\n",
    "        out_w = int((in_w + l_pad + r_pad - self.kernel_size)//self.stride[1] + 1)\n",
    "        out_ch = in_ch\n",
    "        output = np.zeros((in_n,out_h, out_w, out_ch))\n",
    "        self.output_shape = output.shape\n",
    "        for ch in range(out_ch):\n",
    "            for im_region, i, j, ch in self.iterate_regions(input, out_h, out_w, ch):\n",
    "                output[...,i,j,ch] = np.amax(im_region, axis=(1,2))\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def backward(self, d_L_d_out):\n",
    "        in_n,in_h, in_w, in_ch = self.input_shape\n",
    "        u_pad = self.padding[0][0]\n",
    "        d_pad = self.padding[0][1]\n",
    "        l_pad = self.padding[1][0]\n",
    "        r_pad = self.padding[1][1]\n",
    "        \n",
    "        d_L_d_input = np.zeros(self.last_input.shape)\n",
    "        out_h = int((in_h + u_pad + d_pad - self.kernel_size)//self.stride[0] + 1)\n",
    "        out_w = int((in_w + l_pad + r_pad - self.kernel_size)//self.stride[1] + 1)\n",
    "        out_ch = in_ch\n",
    "        for ch in range(out_ch):\n",
    "            for im_region, i, j, ch in self.iterate_regions(self.last_input, out_h, out_w, ch):\n",
    "                n,h,w = im_region.shape\n",
    "                amax = np.amax(im_region, axis=(1,2))\n",
    "                for ni in range(n):\n",
    "                    for i2 in range(h):\n",
    "                        for j2 in range(w):\n",
    "                            if np.all(im_region[ni,i2, j2] == amax[ni,...]):\n",
    "                                d_L_d_input[ni,i*self.stride[0]+i2, j*self.stride[1]+j2,ch]=d_L_d_out[ni,i,j,ch]\n",
    "                            \n",
    "        return d_L_d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    def __init__(self, name, num_filters, activation):\n",
    "        self.name = name\n",
    "        self.num_filters = num_filters\n",
    "        self.activation = activation\n",
    "        with open('initial_weights_lenet/'+self.name+'.weight', 'rb') as fp:\n",
    "            variables = pickle.load(fp)\n",
    "            self.filters = variables[0].numpy()\n",
    "            self.bias = variables[1].numpy()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        \n",
    "        self.filters = np.array(self.filters)\n",
    "        self.bias = np.array(self.bias)\n",
    "        \n",
    "        output = np.zeros((input.shape[0],self.num_filters))\n",
    "        \n",
    "        if len(input.shape) is 4:\n",
    "            flattend_in = input.flatten().reshape(input.shape[0],input.shape[1]*input.shape[2]*input.shape[3])\n",
    "        elif len(input.shape) is 2:\n",
    "            flattend_in = input\n",
    "        \n",
    "        self.input_shape = input.shape\n",
    "        self.cache = flattend_in\n",
    "\n",
    "        for ch in range(self.num_filters):\n",
    "            output[...,ch] = np.sum(flattend_in*self.filters[...,ch], axis=1)+self.bias[ch]\n",
    "            if self.activation == 'relu':\n",
    "                output = np.maximum(output,0)\n",
    "                    \n",
    "        return output\n",
    " \n",
    "    def backward(self, dL_dout, out, learning_rate=0.01,update=True):\n",
    "        #dout_dw = np.zeros((dL_dout.shape[0], self.cache.shape[1], self.num_filters))\n",
    "        #for i,d in enumerate(dL_dout):\n",
    "        #    dout_dw[i,...] = np.repeat(self.cache[i,:,np.newaxis],self.num_filters, axis=1)\n",
    "        #    dL_dw = dout_dw[i,...] * d[np.newaxis,np.newaxis,:]\n",
    "        dL_dact = np.ones(dL_dout.shape) \n",
    "        if self.activation == 'relu':\n",
    "            dL_dact = np.expand_dims(np.where(out>0, 1, 0),axis=1)\n",
    "        dL_dact = dL_dact * dL_dout\n",
    "            \n",
    "        \n",
    "        dL_dw = np.zeros((dL_dout.shape[0],self.cache.shape[1], self.num_filters))\n",
    "        dL_din = np.zeros((self.cache.shape[0], 1, self.cache.shape[1]))\n",
    "        dL_dbias = np.zeros((dL_dout.shape[0], self.bias.shape[0]))\n",
    "        \n",
    "        for n in range(dL_dout.shape[0]):\n",
    "            for i in range(self.num_filters):\n",
    "                dL_dw[n,:,i] = dL_dact[n,:,i] * self.cache[n,:]\n",
    "            dL_dbias[n,...] = dL_dact[n,...]\n",
    "        for i in range(dL_din.shape[0]):\n",
    "            dL_din[i,0,:] = np.sum(dL_dact[i,:,:] * self.filters[:,:],axis=1)\n",
    "\n",
    "        if self.name == 'fc0':\n",
    "            print('{} filters'.format(self.name))\n",
    "            print(self.bias)\n",
    "            print('grads')\n",
    "            print(dL_dbias)\n",
    "        if update:\n",
    "            for n in range(self.cache.shape[0]):  \n",
    "                self.filters -= learning_rate * dL_dw[n]\n",
    "                self.bias -= learning_rate * dL_dbias[n]\n",
    "\n",
    "        return dL_din"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4195,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self, input):\n",
    "        exp = np.exp(input)\n",
    "        prob = exp / np.expand_dims(np.sum(exp, axis=1),axis=1)\n",
    "        self.cache = prob\n",
    "        \n",
    "        return prob\n",
    "    \n",
    "    def backward(self, dL_dout, labels):\n",
    "        dout_din = np.zeros((dL_dout.shape[0], dL_dout.shape[1], self.cache.shape[1]))\n",
    "        for i,(p, l) in enumerate(zip(self.cache, labels)):\n",
    "            dout_din[i] = (-self.cache[i]*self.cache[i][l]) * dL_dout[i]\n",
    "            dout_din[i][0][l] = self.cache[i][l]*(1-self.cache[i][l])* dL_dout[i]\n",
    "        return dout_din\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4196,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "class CrossEntropy:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, predictions, labels):\n",
    "        loss=0.0\n",
    "        for p,l in zip(predictions, labels):\n",
    "            loss = np.float64(Decimal(-np.log(p[l])))\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, predictions, labels):\n",
    "        input_shape = predictions.shape\n",
    "        dL_dout = np.zeros((input_shape[0], 1))\n",
    "        for i,(p,l) in enumerate(zip(predictions,labels)):\n",
    "            dL_dout[i][0] = -1/p[l]\n",
    "        return dL_dout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4197,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output\n",
      "[[0.10315515 0.0828084  0.089298   0.10259349 0.09754225 0.08758666\n",
      "  0.1112497  0.10728637 0.09372789 0.12475208]]\n",
      "loss\n",
      "2.08142688828893\n",
      "conv1 filters\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "grads\n",
      "[[-0.07368661 -0.18158476  0.11003846 -0.26580147 -0.09740951  0.08516611]]\n",
      "model output\n",
      "[[0.10172734 0.07764438 0.08911259 0.09566537 0.09221403 0.09939929\n",
      "  0.10701083 0.11388674 0.09024125 0.13309818]]\n",
      "loss\n",
      "2.285459151272034\n",
      "conv1 filters\n",
      "[ 0.00073687  0.00181585 -0.00110038  0.00265801  0.0009741  -0.00085166]\n",
      "grads\n",
      "[[-0.04435083  0.19257417 -0.09588984  0.04751892 -0.22196138  0.0213418 ]]\n",
      "model output\n",
      "[[0.10505752 0.09077225 0.09642118 0.09976182 0.09310231 0.09788315\n",
      "  0.10053141 0.10240769 0.09307689 0.12098578]]\n",
      "loss\n",
      "2.253247305604628\n",
      "conv1 filters\n",
      "[ 0.00118037 -0.00010989 -0.00014149  0.00218283  0.00319371 -0.00106508]\n",
      "grads\n",
      "[[-0.58939721  0.04575713 -0.27544069 -0.20367812 -0.32374471  0.00046923]]\n",
      "model output\n",
      "[[0.11077128 0.08741872 0.09245274 0.10430553 0.08783333 0.09546447\n",
      "  0.10250258 0.10327585 0.09240398 0.12357153]]\n",
      "loss\n",
      "2.260430909510361\n",
      "conv1 filters\n",
      "[ 0.00707435 -0.00056747  0.00261292  0.00421961  0.00643116 -0.00106977]\n",
      "grads\n",
      "[[ 0.50460453 -0.20564251  0.46508051 -0.0236232   0.24678838 -0.1054043 ]]\n"
     ]
    }
   ],
   "source": [
    "conv1 = Conv('conv1',6,5,((2,2),(2,2)), (1,1))\n",
    "pool1 = MaxPool('pool1',2,padding=((0,0),(0,0)),stride=(2,2))\n",
    "conv2 = Conv('conv2',16,5,((0,0),(0,0)), (1,1))\n",
    "pool2 = MaxPool('pool2',2,padding=((0,0),(0,0)),stride=(2,2))\n",
    "fc1 = FC('fc1', 120, 'relu')\n",
    "fc2 = FC('fc2', 84, 'relu')\n",
    "fc3 = FC('fc3', 10, 'linear')\n",
    "softmax =Softmax()\n",
    "loss = CrossEntropy()\n",
    "\n",
    "image_dataset=tf.data.Dataset.from_tensor_slices(train_images[:4]).batch(1)\n",
    "label_dataset=tf.data.Dataset.from_tensor_slices(train_labels[:4]).batch(1)\n",
    "\n",
    "for i, (x,y) in enumerate(zip(image_dataset, label_dataset)):\n",
    "#conv1_output = conv1.forward(train_images[:4])\n",
    "    conv1_output = conv1.forward(x)\n",
    "    pool1_output = pool1.forward(conv1_output)\n",
    "    conv2_output = conv2.forward(pool1_output)\n",
    "    pool2_output = pool2.forward(conv2_output)\n",
    "    fc1_output = fc1.forward(pool2_output)\n",
    "    fc2_output = fc2.forward(fc1_output)\n",
    "    fc3_output = fc3.forward(fc2_output)\n",
    "    out = softmax.forward(fc3_output)\n",
    "    print('model output')\n",
    "    print(out)\n",
    "    print('loss')\n",
    "    print(loss(out,y))\n",
    "\n",
    "    #dL_dsm = loss.backward(out, train_labels[:4])\n",
    "    #dsm_dfc3 = softmax.backward(dL_dsm, train_labels[:4])\n",
    "    dL_dsm = loss.backward(out, y)\n",
    "    dsm_dfc3 = softmax.backward(dL_dsm, y)\n",
    "    dfc3_dfc2 = fc3.backward(dsm_dfc3, fc3_output,update=True)\n",
    "    dfc2_dfc1 = fc2.backward(dfc3_dfc2,fc2_output,update=True)\n",
    "    dfc1_dpool2 = fc1.backward(dfc2_dfc1, fc1_output,update=True)\n",
    "    dfc1_dpool2 = dfc1_dpool2.reshape((pool2.output_shape[0],pool2.output_shape[1], pool2.output_shape[2], pool2.output_shape[3]))\n",
    "    dpool2_dconv2 = pool2.backward(dfc1_dpool2)\n",
    "    dconv2_dpool1 = conv2.backward(dpool2_dconv2,conv2_output,0.01,update=True)\n",
    "    dpool1_dconv1 = pool1.backward(dconv2_dpool1)\n",
    "    dconv1_dinput = conv1.backward(dpool1_dconv1,conv1_output,0.01,update=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import *\n",
    "import matplotlib.pyplot as plt\n",
    "test_num = randint(1,1000)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(train_images[test_num])\n",
    "print(train_images[test_num].shape)\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "prediction = forward(test_images[test_num], test_labels[test_num])\n",
    "print(test_labels[test_num])\n",
    "print(prediction)\n",
    "print(np.argmax(forward(test_images[test_num], test_labels[test_num])[0]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_general",
   "language": "python",
   "name": "venv_general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
