{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1389,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import sys\n",
    "import numpy as np\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "test_images=np.expand_dims(test_images, axis=3)\n",
    "train_images=np.expand_dims(train_images, axis=3)\n",
    "print(train_images.shape)\n",
    "train_images = train_images /255.0\n",
    "test_images = test_images /255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1391,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(name='conv1',input_shape=(28,28,1),filters=6, kernel_size=5, strides=1, padding='same',activation='relu'),\n",
    "    keras.layers.MaxPool2D(name='pool1',pool_size=2),\n",
    "    keras.layers.Conv2D(name='conv2',filters=16, kernel_size=5, strides=1, padding='valid',activation='relu'),\n",
    "    keras.layers.MaxPool2D(name='pool2',pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(120, activation='relu',name='fc1'),\n",
    "    keras.layers.Dense(84, activation='relu',name='fc2'),\n",
    "    keras.layers.Dense(10,name='fc3'),\n",
    "    keras.layers.Softmax()\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1392,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_222\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "pool1 (MaxPooling2D)         (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (None, 10, 10, 16)        2416      \n",
      "_________________________________________________________________\n",
      "pool2 (MaxPooling2D)         (None, 5, 5, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_190 (Flatten)        (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 120)               48120     \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "fc3 (Dense)                  (None, 10)                850       \n",
      "_________________________________________________________________\n",
      "softmax_191 (Softmax)        (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "{'name': 'conv1', 'trainable': True, 'batch_input_shape': (None, 28, 28, 1), 'dtype': 'float64', 'filters': 6, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'same', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "{'name': 'pool1', 'trainable': True, 'dtype': 'float64', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}\n",
      "{'name': 'conv2', 'trainable': True, 'dtype': 'float64', 'filters': 16, 'kernel_size': (5, 5), 'strides': (1, 1), 'padding': 'valid', 'data_format': 'channels_last', 'dilation_rate': (1, 1), 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "{'name': 'pool2', 'trainable': True, 'dtype': 'float64', 'pool_size': (2, 2), 'padding': 'valid', 'strides': (2, 2), 'data_format': 'channels_last'}\n",
      "{'name': 'flatten_190', 'trainable': True, 'dtype': 'float64', 'data_format': 'channels_last'}\n",
      "{'name': 'fc1', 'trainable': True, 'dtype': 'float64', 'units': 120, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "{'name': 'fc2', 'trainable': True, 'dtype': 'float64', 'units': 84, 'activation': 'relu', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "{'name': 'fc3', 'trainable': True, 'dtype': 'float64', 'units': 10, 'activation': 'linear', 'use_bias': True, 'kernel_initializer': {'class_name': 'GlorotUniform', 'config': {'seed': None}}, 'bias_initializer': {'class_name': 'Zeros', 'config': {}}, 'kernel_regularizer': None, 'bias_regularizer': None, 'activity_regularizer': None, 'kernel_constraint': None, 'bias_constraint': None}\n",
      "{'name': 'softmax_191', 'trainable': True, 'dtype': 'float64', 'axis': -1}\n",
      "(400, 120)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model.load_weights('initial_weights_alexnet_h5')\n",
    "print(model.summary())\n",
    "for l in model.layers:\n",
    "    print(l.get_config())\n",
    "    \n",
    "print(model.layers[5].trainable_variables[0].shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model.compile(optimizer=keras.optimizers.SGD(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=1)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10315515 0.0828084  0.089298   0.10259349 0.09754225 0.08758666\n",
      "  0.1112497  0.10728637 0.09372789 0.12475208]]\n",
      "[9]\n",
      "-2.08142688828892996610875343321822583675384521484375\n"
     ]
    }
   ],
   "source": [
    "from decimal import Decimal\n",
    "def loss(predictions, label):\n",
    "    oss=0.0\n",
    "    loss=predictions[label[0]]\n",
    "    loss = Decimal(np.log(predictions[label[0]]))\n",
    "    return loss\n",
    "\n",
    "predictions = model(train_images[:1])\n",
    "print(predictions.numpy())\n",
    "print(train_labels[:1])\n",
    "print(loss(predictions.numpy()[0], train_labels[:1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1394,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset=tf.data.Dataset.from_tensor_slices(train_images[:4]).batch(1)\n",
    "label_dataset=tf.data.Dataset.from_tensor_slices(train_labels[:4]).batch(1)\n",
    "\n",
    "loss_object=keras.losses.SparseCategoricalCrossentropy(reduction='sum_over_batch_size')\n",
    "\n",
    "def loss(model, x, y, training):\n",
    "    y_ = model(x, training=training)\n",
    "    return loss_object(y_true=y, y_pred=y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_value = loss(model, inputs, targets, training=False)\n",
    "\n",
    "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1395,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output\n",
      "tf.Tensor(\n",
      "[[0.10315515 0.0828084  0.089298   0.10259349 0.09754225 0.08758666\n",
      "  0.1112497  0.10728637 0.09372789 0.12475208]], shape=(1, 10), dtype=float64)\n",
      "loss\n",
      "tf.Tensor(2.0814268589019775, shape=(), dtype=float64)\n",
      "filters\n",
      "<tf.Variable 'conv1_200/bias:0' shape=(6,) dtype=float64, numpy=array([0., 0., 0., 0., 0., 0.])>\n",
      "grads\n",
      "tf.Tensor([-0.07368661 -0.18158476  0.11003846 -0.26580147 -0.09740951  0.08516611], shape=(6,), dtype=float64)\n",
      "model output\n",
      "tf.Tensor(\n",
      "[[0.10172734 0.07764438 0.08911259 0.09566537 0.09221403 0.09939929\n",
      "  0.10701083 0.11388674 0.09024125 0.13309818]], shape=(1, 10), dtype=float64)\n",
      "loss\n",
      "tf.Tensor(2.285459041595459, shape=(), dtype=float64)\n",
      "filters\n",
      "<tf.Variable 'conv1_200/bias:0' shape=(6,) dtype=float64, numpy=\n",
      "array([ 0.00073687,  0.00181585, -0.00110038,  0.00265801,  0.0009741 ,\n",
      "       -0.00085166])>\n",
      "grads\n",
      "tf.Tensor([ 0.00906555  0.21478937 -0.09588984  0.03190288 -0.19209093  0.0213418 ], shape=(6,), dtype=float64)\n",
      "model output\n",
      "tf.Tensor(\n",
      "[[0.10503905 0.09076346 0.09643244 0.0997498  0.09313159 0.09789315\n",
      "  0.10052268 0.10240746 0.09308317 0.1209772 ]], shape=(1, 10), dtype=float64)\n",
      "loss\n",
      "tf.Tensor(2.2534232139587402, shape=(), dtype=float64)\n",
      "filters\n",
      "<tf.Variable 'conv1_200/bias:0' shape=(6,) dtype=float64, numpy=\n",
      "array([ 0.00064621, -0.00033205, -0.00014149,  0.00233899,  0.002895  ,\n",
      "       -0.00106508])>\n",
      "grads\n",
      "tf.Tensor([-0.17661213  0.05531668 -0.26858277 -0.08742061 -0.402756    0.01717068], shape=(6,), dtype=float64)\n",
      "model output\n",
      "tf.Tensor(\n",
      "[[0.11073524 0.0873295  0.09248127 0.10427183 0.08773347 0.09562807\n",
      "  0.10250044 0.10333998 0.0924684  0.1235118 ]], shape=(1, 10), dtype=float64)\n",
      "loss\n",
      "tf.Tensor(2.260754108428955, shape=(), dtype=float64)\n",
      "filters\n",
      "<tf.Variable 'conv1_200/bias:0' shape=(6,) dtype=float64, numpy=\n",
      "array([ 0.00241233, -0.00088521,  0.00254434,  0.00321319,  0.00692256,\n",
      "       -0.00123679])>\n",
      "grads\n",
      "tf.Tensor([ 0.04840934 -0.21305673 -0.08663555  0.0252186   0.04172996 -0.08889536], shape=(6,), dtype=float64)\n",
      "Epoch 000: Loss: 0.000, Accuracy: 0.000%\n"
     ]
    }
   ],
   "source": [
    "input=train_images[:1000]\n",
    "label=train_labels[:1000]\n",
    "\n",
    "#newModel2.save_weights('initial_weights_h5',save_format='h5')    \n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "num_epochs = 1\n",
    "newModel = keras.Sequential(model.layers[:1])\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    epoch_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    # Training loop - using batches of 32\n",
    "    for i, (x, y) in enumerate(zip(image_dataset, label_dataset)):\n",
    "    # Optimize the model\n",
    "        input=x\n",
    "        label=y\n",
    "        loss_value, grads = grad(model, input, label)\n",
    "        \n",
    "        y_ = model(input)\n",
    "        print('model output')\n",
    "        print(y_)\n",
    "        print('loss')\n",
    "        print(loss_value)\n",
    "        print('filters')\n",
    "        print(model.layers[0].trainable_variables[1])\n",
    "        print('grads')\n",
    "\n",
    "        \n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        print(grads[1])\n",
    "        # Track progress\n",
    "        #epoch_loss_avg.update_state(loss_value)  # Add current batch loss\n",
    "        # Compare predicted label to actual label\n",
    "        # training=True is needed only if there are layers with different\n",
    "        # behavior during training versus inference (e.g. Dropout).\n",
    "        y_ = model(input)\n",
    "\n",
    "        #epoch_accuracy.update_state(label, y_)\n",
    "    \n",
    "\n",
    "    # End epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "newModel2.save_weights('weights__lenet_h5',save_format='h5')\n",
    "for l in newModel.layers:\n",
    "    if l.trainable_variables:\n",
    "        variables = l.trainable_variables\n",
    "        if l.name.split('_')[-1] == 'bn':\n",
    "            variables.append(l.moving_mean)\n",
    "            variables.append(l.moving_variance)\n",
    "        with open('weights_lenet/'+l.name+'.weight',\"wb\") as fp:\n",
    "            pickle.dump(variables,fp)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])\n",
    "predictions = probability_model.predict(test_images)\n",
    "%matplotlib inline\n",
    "from random import *\n",
    "import matplotlib.pyplot as plt\n",
    "test_num = randint(1,1000)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.squeeze(test_images[test_num],axis=2))\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "label = np.argmax(predictions[test_num])\n",
    "print(class_names[label], class_names[test_labels[test_num]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_general",
   "language": "python",
   "name": "venv_general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
